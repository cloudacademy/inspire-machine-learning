{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"spam_detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYjL2wlv+GxYGpH8ab20Gw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Spam detection using Transformers Models\n","\n","In this notebook, we will leverage transformers models such as BERT to classify whether an SMS contains spam or not.\n","\n","\n","### * What is a Transformer model?\n","\n","A transformer is a deep learning model that adopts the mechanism of self-attention. It is used primarily in the fields of **Natural Language Processing (NLP)** and Computer Vision (CV). \n","\n","**BERT** (Bidirectional Encoder Representations from Transformers) is one of the most famous transformer models released by Google in 2018.\n","\n","### * Which family of Machine Learning paradigms does this task (spam detection) belong to?\n","\n","The task of detecting if an SMS contains spam or not is a **supervised** task. Each example is associated with a **label** (1: SPAM, 0: not SPAM). Since the labels are discrete, they are classes, the task is a **classification** (binary classification because #classes = 2)\n","\n","### * Which libraries are we going to use?\n","\n","We are going to use one of the most famous libraries to work with Transformers model, the name is the library is (you do not need a lot of fantasy) *transformers* by *HuggingFace*.\n"],"metadata":{"id":"BrYUOKDs1ny_"}},{"cell_type":"markdown","source":["# Libraries installation"],"metadata":{"id":"nrYVbx4rFFIF"}},{"cell_type":"code","source":["!pip install transformers --quiet"],"metadata":{"id":"ERdi2P_Fjsf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install datasets --quiet"],"metadata":{"id":"9BowahPAtmsT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pandas -U --quiet"],"metadata":{"id":"P5_5xiOhkFeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet shap"],"metadata":{"id":"qIboJT5tt_nQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"7ee3_yanFOmQ"}},{"cell_type":"markdown","source":["The dataset is hosted in the *Huggingface* dataset hub and can be easly downloaded."],"metadata":{"id":"OLVRcpsgH0QU"}},{"cell_type":"code","source":["from datasets import load_dataset\n","# https://huggingface.co/datasets/sms_spam\n","spam_dataset = load_dataset(\"sms_spam\", split = [\"train\"])"],"metadata":{"id":"7UQ2L0zltQwt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset is composed by 5574 examples and has 2 columns:\n","- *sms*: the text of the sms.\n","- *label*: 1 if spam, 0 otherwise."],"metadata":{"id":"u1RxDKVIHcqU"}},{"cell_type":"code","source":["spam_dataset[0]"],"metadata":{"id":"AMJuSYrquI9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's have a look at some rows..."],"metadata":{"id":"XYAV2hbxH-wZ"}},{"cell_type":"code","source":["spam_dataset[0][100]"],"metadata":{"id":"lSjpP21YvUfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spam_dataset[0][2]"],"metadata":{"id":"_vBo_4GwIw3I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How are the labels distributed? Let's see if the dataset is unbalanced."],"metadata":{"id":"MLkABtn9O7yC"}},{"cell_type":"code","source":["from collections import Counter\n","Counter([sample['label'] for sample in spam_dataset[0]])"],"metadata":{"id":"RfQV4dPeN1T4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"P3JMzPOAFbQy"}},{"cell_type":"markdown","source":["The model is hosted in the *Huggingface* model hub and can be easily downloaded.\n","\n","Someone (thanks) already trained (better to say finetuned) a BERT model using the dataset we have seen above. Thus we are not focusing on the training part, but remember that training a neural network requires time and resources (GPU/TPU). One of the pros of using the transformers library is that: \n","* Researchers can share trained models instead\n","* Practitioners can reduce compute time and production costs\n"],"metadata":{"id":"WqBBmvJtKGUV"}},{"cell_type":"markdown","source":["## Bert \n","\n","Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018.\n","\n","**Architecture**\n","\n","The original English-language BERT has been released into 2 versions:\n","- BERT BASE: 12 encoders with 12 bidirectional - self-attention heads. (110 M parameters)\n","- BERT LARGE: 24 encoders with 16 bidirectional - self-attention heads. (345 M parameters)\n","\n","**Data**\n","\n","Both models are pre-trained from unlabeled data extracted from the BooksCorpus with 800M words and English Wikipedia with 2,500M words.\n","\n","**Tasks**\n","\n","BERT was pre-trained on two tasks (self-supervised): \n","\n","- Masked Language Modelling: 15% of tokens were masked and BERT was trained to predict them from context. (You can try it [here](https://huggingface.co/bert-base-uncased) to better understand this task).\n","- Next Sentence Prediction: BERT was trained to predict if a chosen next sentence was probable or not given the first sentence. \n","\n","**Result**\n","\n","After pretraining, which is computationally expensive, BERT can be finetuned with fewer resources on smaller datasets to optimize its performance on specific tasks.\n"],"metadata":{"id":"Evip9IPqPqiJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TV2LjOu1k1aj"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# the name of a model (that is on the model hub is this case)\n","model_name = \"mariagrandury/distilbert-base-uncased-finetuned-sms-spam-detection\" #mariagrandury/distilbert-base-uncased-finetuned-sms-spam-detection\n","\n","# let's load the model \n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","# let's load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","source":["## Tokenizer\n","\n","A tokenizer is in charge of preparing the inputs for the model. When the BERT model was trained, each token was given a unique ID. Therefore, when we want to use a pre-trained BERT model, we will first need to convert each token in the input sentence into its corresponding unique IDs.\n","\n","\n","BERT uses a WordPiece algorithm that breaks a word into several subwords: we cannot say word = token.\n","\n","BERT has some special token that are: [CLS], [SEP], [PAD] and [UNK]."],"metadata":{"id":"7387o_oHl4BZ"}},{"cell_type":"code","source":["sample = 'This is an example to show you how is done the tokenization process.'\n","encoding = tokenizer.encode(sample)\n","print(encoding)\n","print(tokenizer.convert_ids_to_tokens(encoding))"],"metadata":{"id":"wtWv-3PnLk5q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Testing**"],"metadata":{"id":"Jk6oDZUsNMne"}},{"cell_type":"markdown","source":["Legend: \n","\n","* LABEL_1 -> spam\n","\n","* LABEL_2 -> not spam\n","\n","* score -> how much \"\"confident\"\" the model is about the predicted label [0;1]"],"metadata":{"id":"eHL1e1ntefOj"}},{"cell_type":"code","source":["from transformers import TextClassificationPipeline\n"," # a pipeline is very easy-to-use abstraction, which require as little as two lines of code to perform a prediction given a model and a tokenizer\n","classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n","classifier(\"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\")"],"metadata":{"id":"qfq9juu8jodH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try it"],"metadata":{"id":"dEHR0HucXB4A"}},{"cell_type":"code","source":["# good prediction :)\n","classifier(\"Hey John, do you like Machine Learning?\")"],"metadata":{"id":"vLaOUrpWs2AH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# good prediction :)\n","classifier(\"Urgent! you are the selected winner of 1 bitcoin, answer YES to confirm your price.\")"],"metadata":{"id":"TnXSxecjs_Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# good prediction :)\n","classifier(\"Hey Luis, you call Peter at 093232141? I'm in a meeting right now, see you later.\")"],"metadata":{"id":"TmTBUvnptV01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bad prediction :(\n","classifier(\"Answer YES, to get the chance to win a Ferrari, 2$ per month.\")"],"metadata":{"id":"nK3-dJKEXuBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics\n","\n","WARNING: The data we are testing likely has been used to train the model (we should use a different split of the dataset that we don't have) but the goal is to show you how to calculate the metric not looking at the specific number"],"metadata":{"id":"l6OdwerIiIwO"}},{"cell_type":"code","source":["references = [sample['label'] for sample in spam_dataset[0]][:1000]\n","input_texts = [sample['sms'] for sample in spam_dataset[0]][:1000]"],"metadata":{"id":"HJ7DbpEnzsOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = classifier(input_texts)"],"metadata":{"id":"VfhtTgzD0oKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_mapping = {\"LABEL_0\": 0, \"LABEL_1\": 1}\n","predictions = [label_mapping[pred_dict[\"label\"]] for pred_dict in predictions]"],"metadata":{"id":"fmJ3iBJL1KTH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's review some metrics for classification: \n","\n","$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n","\n","$Precision = \\frac{TP}{TP+FP}$\n","\n","$Recall = \\frac{TP}{TP+FN}$\n","\n","$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$\n","\n","\n","\n","\n","\n"],"metadata":{"id":"h4FsreOS2TTC"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","load_f1 = load_metric(\"accuracy\")\n","accuracy = load_f1.compute(predictions=predictions, references=references)[\"accuracy\"]\n","print(\"Accuracy: \", accuracy)"],"metadata":{"id":"N_fsMnGR109e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset is quite imbalanced, it is better to look at the F1 score"],"metadata":{"id":"gourP_o825Rg"}},{"cell_type":"code","source":["load_f1 = load_metric(\"f1\")\n","f1 = load_f1.compute(predictions=predictions, references=references)[\"f1\"]\n","print(\"F1 score: \", f1)"],"metadata":{"id":"NOJKQw1hpKkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explanability\n"],"metadata":{"id":"-kC_LJTjJ145"}},{"cell_type":"markdown","source":["## What if we don't want just a number but a deeper understanding of which part of the text influenced the prediction?\n","\n","Shap to the rescue"],"metadata":{"id":"3xJnkjCzYRv9"}},{"cell_type":"code","source":["import shap\n","explainer = shap.Explainer(classifier)\n","shap_values = explainer([\"Urgent! you are the selected winner of 1 bitcoin, answer YES to confirm your price.\"])"],"metadata":{"id":"LN0RNy4XuTx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.plots.text(shap_values[:,:,\"LABEL_1\"])"],"metadata":{"id":"AkdItYdbulxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.plots.bar(shap_values[0,:,\"LABEL_1\"])"],"metadata":{"id":"PjLEjwfbroz5"},"execution_count":null,"outputs":[]}]}